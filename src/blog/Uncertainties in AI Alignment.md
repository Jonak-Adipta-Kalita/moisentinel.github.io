---
aliases: 
tags:
  - post
  - technical
  - rationality
added: September 16, 2024
link: https://nibirsan.org/blog/p/uncertainties-in-ai-alignment
excerpt: 
date: 
status: needs work
content-type: blog
---
(Draft) Identifying Uncertainties in AI; AGISF Session 2 exercise.
## (Possible) Solutions to AI Safety
### Resolve human values
As I will later talk about this in the post, morality is subjective. Even *we* humans are not sure of what's right and what's wrong.

Rationalists are fighting against Empiricists. [Engineers against Philosophers](https://bounded-regret.ghost.io/more-is-different-for-ai/). We must align *ourselves* before aligning AI systems. That means, we have to be more rational, more agreeable to the *universal truth*; at least the industry leaders and makers.
### Government intervention
It is possible that government intervention: safety rules and regulations, can mitigate the risk of *deployment* of misaligned AI systems and misuse of AI.

So far, AI developments are outpacing government regulations and law-making. Only the EU seems to [have done something](https://www.deloitte.com/nl/en/services/legal/perspectives/the-EU-Artificial-Intelligence-Act-deep-dive.html).
### Transparency
The proprietary AI models lead the market, but that is also a cause of concern. As an open-source endorser, I feel that making the models (and what goes behind) a bit more transparent could make things much better.

I do understand that there are a lot of factors including the economic that make it impossible. But perhaps there can be a *communication bridge*, at least between companies, that lets them "check and balance" each other's work.
### More resources in AI safety research
Today, the majority of the research being done is in search of AI systems' *capabilities*, although that seems to be changing. And it should!

We should put in more and and more resources into AI safety research. This will be the most *effective* measure for a safer future,
### More resources in experimental fields
Perhaps studying Digital Neuroscience or similar experimental fields can yield good results.
## Human-level AI
It is possible for human-level AI to come into existence in the next 10-12 years given the current rate of development. Other forecasters have also predicted that it is possible to get such a AI system before 2040:

{% lanimage "src/blog/attachments/human-level-AI.png", "" %}

[source](https://www.metaculus.com/questions/384/humanmachine-intelligence-parity-by-2040/) {.caption}

The question is if it will be safe or not. because you *can* make a very intelligent machine by training it with a large dataset and a good optimisation algorithm *without* putting an effort to align it properly. 

Recent developments in AI models, such as OpenAI's o1 model is capable of System-2 thinking, which is *one* step closer to an AGI. 
And which is capable of displaying such feats:

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Fucking wild.<a href="https://twitter.com/OpenAI?ref_src=twsrc%5Etfw">@OpenAI</a>&#39;s new o1 model was tested with a Capture The Flag (CTF) cybersecurity challenge. But the Docker container containing the test was misconfigured, causing the CTF to crash. Instead of giving up, o1 decided to just hack the container to grab the flag inside.… <a href="https://t.co/5FaqQY6bok">pic.twitter.com/5FaqQY6bok</a></p>&mdash; Haseeb ＞|＜ (@hosseeb) <a href="https://twitter.com/hosseeb/status/1834378405896401380?ref_src=twsrc%5Etfw">September 12, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

And that is somewhat *concerning*, and partly confirms the following prediction 

{% lanimage "src/blog/attachments/unauth-AI.png", "" %}

[source](https://www.metaculus.com/questions/16014/ai-unauthorized-access-before-2033/) {.caption}

But in a 100 years... let's hope things don't turn out something like Terminator.
## Pursuing convergent instrumental goals
Will the AI systems that we build pursue convergent instrumental goals? Maybe. 

The model needs to have some sort of situational awareness to be able to do that. So, it is highly probable for "Schemer" AI models to do [refuse orders](https://www.youtube.com/watch?v=Mme2Aya_6Bc), try to get more power, manipulate, etc. all to marginally increase the odds of completing its original goal.

Say, we prompted an AGI to "create a better AI alignment procedure". It would soon realise that we're aligning AI's to conform to humane ideals, and the easiest way to "align" would be to kill all the humans! (now they'll be all *aligned* to a null and void) ???

It would be very interesting to see how OpenAI's o1 performs in this regard. Check it's system card [here](https://openai.com/index/openai-o1-system-card/).
## Safe by default
Will AI systems we build be safe by default? No, I am not sure. Maybe not in today's AI systems, but perhaps we'll see more safer-by-default systems with time. 

Why not today? One of the reasons is our current model training strategy. It is flawed. We can't really understand most of the things, and it makes it really difficult.

Why tomorrow? Because of our efforts in alignment research. Perhaps it will turn out well. But that's only hope, speculation.
## One AI system? or more?
It depends. It's not hard to imagine the case where AGIs against each other for power (battle royale), make allies, etc. Then we'll have one giant ecosystem or a fleet of *one* AGI (assuming we're not made slaves or killed beforehand). 

But it is also probable that, in a more optimistic future, we'll see lots of different, powerful AI systems, tuned to do different specialised tasks. That might be the case if you *limit* AI's capabilities. 
## Align AI systems to what?
AI systems should be aligned to human values.

Only the problem is, "human" values are not necessarily "humane". Morality is subjective (as it has always been). We must know what we mean by morality and alignment in *humans* before aligning the AI systems to our values. Thus, our statement changes to "AI systems should be aligned to *humane* values".

The majority of the AI systems' values and intentions are governed by its stakeholders (which may or may not have good intentions and values). We *could* say, "safety is more important than money, you selfish prick!" but that might not be very practical. Because one of the many reasons of building AI systems is economic growth and there's a huge market for that. One motivation for people to continue pursuing this market *rashly* is that "if we don't do it, someone else will. What makes a difference?". 

In that case, there should be government intervention preventing stakeholders to carry inappropriate practices, such as policy-making and a requirement for "safety".